{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "968f8a36-47b6-4bcf-9962-23f18eaf66e0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fetch Seismic Data from the IRIS API"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the IRIS API endpoint\n",
    "iris_api_url = \"https://service.iris.edu/fdsnws/event/1/query\"\n",
    "\n",
    "# Define search parameters (Only 2024)\n",
    "params = {\n",
    "    \"format\": \"text\",  # Use \"text\" format (IRIS requirement)\n",
    "    \"starttime\": \"2018-01-01\",  # Start of 2018\n",
    "    \"endtime\": \"2024-12-31\",  # End of 2024\n",
    "    \"minmagnitude\": 1.0,  # Include all earthquakes from magnitude 1.0+\n",
    "    \"nodata\": 404  # Return 404 if no data is found\n",
    "}\n",
    "\n",
    "# Send the API request\n",
    "response = requests.get(iris_api_url, params=params)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched seismic data for 2024.\")\n",
    "    print(\"First 500 characters of response:\\n\", response.text[:500])  # Show preview of data\n",
    "elif response.status_code == 404:\n",
    "    print(\"No earthquake events found for 2024.\")\n",
    "else:\n",
    "    print(f\"API request failed with status code {response.status_code}\")\n",
    "    print(response.text)  # Print full error response for debugging\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c92530e9-62c1-4247-9447-f8b490c39aa5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert API Data to a Spark DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session (if not already created)\n",
    "spark = SparkSession.builder.appName(\"SeismicDataProcessing\").getOrCreate()\n",
    "\n",
    "# Extract text data from response\n",
    "raw_text_data = response.text\n",
    "\n",
    "# Convert text to Pandas DataFrame (for initial parsing)\n",
    "df_pandas = pd.read_csv(StringIO(raw_text_data), sep=\"|\")\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "# Show the first few rows\n",
    "print(\"âœ… Data successfully converted to Spark DataFrame!\")\n",
    "df_spark.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "065d592c-52e1-4ad5-ba68-865ff7fd9e28",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Rename Columns so it can be loaded into deltalake"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Rename columns to remove invalid characters\n",
    "df_spark_clean = df_spark.select(\n",
    "    [col(c).alias(c.strip().replace(\"#\", \"\").replace(\" \", \"_\")) for c in df_spark.columns]\n",
    ")\n",
    "\n",
    "# Show the cleaned DataFrame (to verify column names are correct)\n",
    "df_spark_clean.printSchema()\n",
    "df_spark_clean.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7af7df0-a5e8-41bd-9954-25f7f612eb9a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Table Name"
    }
   },
   "outputs": [],
   "source": [
    "# Define the table name\n",
    "\n",
    "table_name = \"iris_seismic_events_bronze\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90bb3578-c684-4e1a-a3ee-ed875e26edfc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "This will drop the table if it exists"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the existing table if it exists\n",
    "# This is because when we will be orchestrating it it is good to delete the old and load the new.\n",
    "# Truncate the table to remove all rows but keep structure & permissions\n",
    "spark.sql(f\"TRUNCATE TABLE tabular.dataexpert.{table_name}\")\n",
    "\n",
    "print(f\"Truncated existing table (if any): {table_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "569dad7e-b214-4179-90c2-4e5dc9d5b0a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Store Data in Delta Table"
    }
   },
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame as a Delta Table inside `tabular.dataexpert`\n",
    "df_spark_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"tabular.dataexpert.{table_name}\")\n",
    "\n",
    "print(f\"Data successfully stored in Delta Table: tabular.dataexpert.{table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d04b233-5043-4376-a5df-b25b5efa6e85",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "validate data has been stored"
    }
   },
   "outputs": [],
   "source": [
    "# Read the stored Delta Table from `tabular.dataexpert`\n",
    "df_check = spark.read.table(f\"tabular.dataexpert.{table_name}\")\n",
    "\n",
    "# Show first 5 rows\n",
    "if df_check.count() > 0:\n",
    "    df_check.show(5)\n",
    "else:\n",
    "    print(\"No earthquake events found in the table.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hist_load_iris_earthquake_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
