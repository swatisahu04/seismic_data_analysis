{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58148a88-5be5-4cdf-8ca2-ad0a63f1ae27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import calendar\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year as spark_year, month as spark_month\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, LongType, IntegerType\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"SeismicDataIngestion\").getOrCreate()\n",
    "\n",
    "# Define Schema\n",
    "schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), False),\n",
    "    StructField(\"magnitude\", DoubleType(), True),\n",
    "    StructField(\"place\", StringType(), True),\n",
    "    StructField(\"time\", TimestampType(), True),\n",
    "    StructField(\"updated\", LongType(), True),\n",
    "    StructField(\"timezone\", IntegerType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"detail\", StringType(), True),\n",
    "    StructField(\"felt\", IntegerType(), True),\n",
    "    StructField(\"cdi\", DoubleType(), True),\n",
    "    StructField(\"mmi\", DoubleType(), True),\n",
    "    StructField(\"alert\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"tsunami\", IntegerType(), True),\n",
    "    StructField(\"significance\", IntegerType(), True),\n",
    "    StructField(\"network\", StringType(), True),\n",
    "    StructField(\"code\", StringType(), True),\n",
    "    StructField(\"ids\", StringType(), True),\n",
    "    StructField(\"sources\", StringType(), True),\n",
    "    StructField(\"types\", StringType(), True),\n",
    "    StructField(\"nst\", IntegerType(), True),\n",
    "    StructField(\"dmin\", DoubleType(), True),\n",
    "    StructField(\"rms\", DoubleType(), True),\n",
    "    StructField(\"gap\", DoubleType(), True),\n",
    "    StructField(\"magType\", StringType(), True),\n",
    "    StructField(\"propertyType\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"depth\", DoubleType(), True),\n",
    "    StructField(\"source\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define Year Range for Backfilling\n",
    "years = range(2018, 2024)  # Backfilling from 2018-2024\n",
    "#years = range(2023, 2024)\n",
    "batch_size = 1  # Load data in 6-month batches to avoid API limits\n",
    "\n",
    "# Loop through Years & Months\n",
    "for year in years:\n",
    "    for month in range(1, 13):\n",
    "        last_day = calendar.monthrange(year, month)[1]\n",
    "        start_time = f\"{year}-{month:02d}-01\"\n",
    "        end_time = f\"{year}-{month:02d}-{last_day}\"\n",
    "\n",
    "        print(f\"Fetching data from {start_time} to {end_time}\")\n",
    "\n",
    "        # USGS API Call (Batch Fetching)\n",
    "        usgs_url = f\"https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={start_time}&endtime={end_time}\"\n",
    "        response = requests.get(usgs_url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "        elif response.status_code == 400:\n",
    "            print(f\"Bad Request: {response.text}\")\n",
    "            #trying to get the data in chunks\n",
    "            chunk_data = []\n",
    "            for day in range(1, last_day + 1, 7):  # Fetching in weekly chunks\n",
    "                    chunk_start = f\"{year}-{month:02d}-{day:02d}\"\n",
    "                    chunk_end = f\"{year}-{month:02d}-{min(day+6, last_day):02d}\"  # Ensures we don't exceed the month's last day\n",
    "                    print(f\"Fetching chunk: {chunk_start} to {chunk_end}\")\n",
    "                    chunk_url = f\"https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={chunk_start}&endtime={chunk_end}\"\n",
    "                    chunk_response = requests.get(chunk_url)\n",
    "                    if chunk_response.status_code == 200:\n",
    "                        chunk_data.extend(chunk_response.json()[\"features\"])\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch data for {chunk_start} to {chunk_end} using {chunk_url}: {chunk_response.status_code}\")\n",
    "                        continue\n",
    "            data = {\"features\": chunk_data}\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data from {start_time} to {end_time} using {usgs_url}: {response.status_code}\")\n",
    "            data = None\n",
    "\n",
    "        # Extract Data\n",
    "        earthquake_records = []\n",
    "        if data:\n",
    "            # Extract Data\n",
    "            earthquake_records = []\n",
    "            for feature in data[\"features\"]:\n",
    "                props = feature[\"properties\"]\n",
    "                geometry = feature[\"geometry\"]\n",
    "                earthquake_records.append({\n",
    "                    \"event_id\": feature[\"id\"],\n",
    "                    \"magnitude\": props.get(\"mag\"),\n",
    "                    \"place\": props.get(\"place\"),\n",
    "                    \"time\": props.get(\"time\"),  # Epoch time\n",
    "                    \"updated\": props.get(\"updated\"),\n",
    "                    \"timezone\": props.get(\"tz\"),\n",
    "                    \"url\": props.get(\"url\"),\n",
    "                    \"detail\": props.get(\"detail\"),\n",
    "                    \"felt\": props.get(\"felt\"),\n",
    "                    \"cdi\": props.get(\"cdi\"),\n",
    "                    \"mmi\": props.get(\"mmi\"),\n",
    "                    \"alert\": props.get(\"alert\"),\n",
    "                    \"status\": props.get(\"status\"),\n",
    "                    \"tsunami\": props.get(\"tsunami\"),\n",
    "                    \"significance\": props.get(\"sig\"),\n",
    "                    \"network\": props.get(\"net\"),\n",
    "                    \"code\": props.get(\"code\"),\n",
    "                    \"ids\": props.get(\"ids\"),\n",
    "                    \"sources\": props.get(\"sources\"),\n",
    "                    \"types\": props.get(\"types\"),\n",
    "                    \"nst\": props.get(\"nst\"),\n",
    "                    \"dmin\": props.get(\"dmin\"),\n",
    "                    \"rms\": props.get(\"rms\"),\n",
    "                    \"gap\": props.get(\"gap\"),\n",
    "                    \"magType\": props.get(\"magType\"),\n",
    "                    \"propertyType\": props.get(\"type\"),\n",
    "                    \"title\": props.get(\"title\"),\n",
    "                    \"longitude\": geometry[\"coordinates\"][0],\n",
    "                    \"latitude\": geometry[\"coordinates\"][1],\n",
    "                    \"depth\": geometry[\"coordinates\"][2],\n",
    "                    \"source\": \"USGS\"\n",
    "                })\n",
    "\n",
    "            # Convert to Pandas DataFrame\n",
    "            df_usgs = pd.DataFrame(earthquake_records)\n",
    "            df_usgs[\"time\"] = pd.to_datetime(df_usgs[\"time\"], unit='ms')\n",
    "\n",
    "            # Convert to PySpark DataFrame\n",
    "            df_spark = spark.createDataFrame(df_usgs, schema=schema)\n",
    "\n",
    "            # Partition by Year & Month\n",
    "            df_spark = df_spark.withColumn(\"year\", spark_year(col(\"time\"))).withColumn(\"month\", spark_month(col(\"time\")))\n",
    "\n",
    "            # Append to Delta Table in Partitions\n",
    "            df_spark.write.format(\"delta\").mode(\"append\").partitionBy(\"year\", \"month\", \"source\").saveAsTable(\"tabular.dataexpert.usgs_seismic_events_bronze\")\n",
    "\n",
    "            print(f\"Data from {start_time} to {end_time} loaded successfully!\")\n",
    "\n",
    "count_df = spark.sql(\"SELECT COUNT(*) AS total_count FROM tabular.dataexpert.usgs_seismic_events_bronze\")\n",
    "#display(count_df)\n",
    "print(f\"USGS Backfill Completed with {count_df} records\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bdc8723-3567-4d4d-9da5-84dc0b747b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count_df = spark.sql(\"SELECT COUNT(*) AS total_count FROM tabular.dataexpert.usgs_seismic_events_bronze\")\n",
    "print(f\"USGS Backfill Completed with {count_df} records\")\n",
    "display(count_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hist_load_usgs_earthquake_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
